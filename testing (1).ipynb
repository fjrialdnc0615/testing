{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9a4d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold', 'SalePrice', 'Street', 'CentralAir'],\n",
      "      dtype='object')\n",
      "Feature: 0, Score: -165.32738\n",
      "Feature: 1, Score: -10.93222\n",
      "Feature: 2, Score: 0.39333\n",
      "Feature: 3, Score: 17780.97136\n",
      "Feature: 4, Score: 5163.61288\n",
      "Feature: 5, Score: 361.69851\n",
      "Feature: 6, Score: 104.68334\n",
      "Feature: 7, Score: 31.98196\n",
      "Feature: 8, Score: 10.13199\n",
      "Feature: 9, Score: -0.07377\n",
      "Feature: 10, Score: -0.73542\n",
      "Feature: 11, Score: 9.32280\n",
      "Feature: 12, Score: 23.56978\n",
      "Feature: 13, Score: 24.27185\n",
      "Feature: 14, Score: -26.98952\n",
      "Feature: 15, Score: 20.85211\n",
      "Feature: 16, Score: 8918.63403\n",
      "Feature: 17, Score: 1237.12818\n",
      "Feature: 18, Score: 4979.37578\n",
      "Feature: 19, Score: -1946.36295\n",
      "Feature: 20, Score: -8980.94161\n",
      "Feature: 21, Score: -19422.31151\n",
      "Feature: 22, Score: 5068.37784\n",
      "Feature: 23, Score: 4360.93568\n",
      "Feature: 24, Score: -13.61506\n",
      "Feature: 25, Score: 16637.27739\n",
      "Feature: 26, Score: -2.47627\n",
      "Feature: 27, Score: 31.22526\n",
      "Feature: 28, Score: -17.84199\n",
      "Feature: 29, Score: 17.55440\n",
      "Feature: 30, Score: 71.26034\n",
      "Feature: 31, Score: 50.43924\n",
      "Feature: 32, Score: -23.82353\n",
      "Feature: 33, Score: -0.14477\n",
      "Feature: 34, Score: -122.06654\n",
      "Feature: 35, Score: -919.62510\n",
      "Feature: 36, Score: 25266.81930\n",
      "Feature: 37, Score: -9059.50581\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "all_data = pd.read_csv('data.csv')\n",
    "data_num = len(all_data)\n",
    "\n",
    "fold_num = 5\n",
    "k_fold = data_num // fold_num\n",
    "\n",
    "\n",
    "#for feature importance\n",
    "temp_data = all_data.iloc[:]\n",
    "numeral_temp_data = temp_data._get_numeric_data()\n",
    "numeral_temp_data = numeral_temp_data.drop(columns='Id')\n",
    "numeral_temp_data['Street'] = temp_data['Street']\n",
    "numeral_temp_data['CentralAir'] = temp_data['CentralAir']\n",
    "\n",
    "numeral_temp_data.Street[numeral_temp_data.Street == 'Pave'] = 1\n",
    "numeral_temp_data.Street[numeral_temp_data.Street == 'Grvl'] = 0\n",
    "\n",
    "numeral_temp_data.CentralAir[numeral_temp_data.CentralAir == 'Y'] = 1\n",
    "numeral_temp_data.CentralAir[numeral_temp_data.CentralAir == 'N'] = 0\n",
    "numeral_temp_data = numeral_temp_data.fillna(0)\n",
    "print(numeral_temp_data.columns)\n",
    "\n",
    "validation_set = numeral_temp_data.iloc[0*k_fold:(1)*k_fold]\n",
    "index_list = validation_set.index\n",
    "training_set = numeral_temp_data.drop(index_list)\n",
    "\n",
    "X_train = training_set.iloc[:].drop(columns='SalePrice')\n",
    "y_train = training_set.iloc[:]['SalePrice']\n",
    "X_test = validation_set.iloc[:].drop(columns='SalePrice')\n",
    "y_test = validation_set.iloc[:][\"SalePrice\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "importance = model.coef_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5ca9c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold\n",
      "Epoch 1/150\n",
      "292/292 [==============================] - 1s 1ms/step - loss: 0.6649\n",
      "Epoch 2/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.2203\n",
      "Epoch 3/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1359\n",
      "Epoch 4/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1106\n",
      "Epoch 5/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1067\n",
      "Epoch 6/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0976\n",
      "Epoch 7/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0934\n",
      "Epoch 8/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0991\n",
      "Epoch 9/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0955\n",
      "Epoch 10/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0907\n",
      "Epoch 11/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0852\n",
      "Epoch 12/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0831\n",
      "Epoch 13/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0817\n",
      "Epoch 14/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0837\n",
      "Epoch 15/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0846\n",
      "Epoch 16/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0856\n",
      "Epoch 17/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0868\n",
      "Epoch 18/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0966\n",
      "Epoch 19/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 20/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0755\n",
      "Epoch 21/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0778\n",
      "Epoch 22/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0787\n",
      "Epoch 23/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0740\n",
      "Epoch 24/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0814\n",
      "Epoch 25/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0710\n",
      "Epoch 26/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0788\n",
      "Epoch 27/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "Epoch 28/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0762\n",
      "Epoch 29/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 30/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0724\n",
      "Epoch 31/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0738\n",
      "Epoch 32/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0727\n",
      "Epoch 33/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0699\n",
      "Epoch 34/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0709\n",
      "Epoch 35/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0762\n",
      "Epoch 36/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0734\n",
      "Epoch 37/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0744\n",
      "Epoch 38/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0753\n",
      "Epoch 39/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0726\n",
      "Epoch 40/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0738\n",
      "Epoch 41/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0748\n",
      "Epoch 42/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0767\n",
      "Epoch 43/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0742\n",
      "Epoch 44/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0710\n",
      "Epoch 45/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0728\n",
      "Epoch 46/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0660\n",
      "Epoch 47/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0698\n",
      "Epoch 48/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0696\n",
      "Epoch 49/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0641\n",
      "Epoch 50/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "Epoch 51/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0659\n",
      "Epoch 52/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0688\n",
      "Epoch 53/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0654\n",
      "Epoch 54/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0653\n",
      "Epoch 55/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0657\n",
      "Epoch 56/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0643\n",
      "Epoch 57/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0678\n",
      "Epoch 58/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 59/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0699\n",
      "Epoch 60/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 61/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 62/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 63/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0662\n",
      "Epoch 64/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0642\n",
      "Epoch 65/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0596\n",
      "Epoch 66/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0604\n",
      "Epoch 67/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0601\n",
      "Epoch 68/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0599\n",
      "Epoch 69/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0595\n",
      "Epoch 70/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0614\n",
      "Epoch 71/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 72/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0584\n",
      "Epoch 73/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0625\n",
      "Epoch 74/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 75/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0612\n",
      "Epoch 76/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 77/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0523\n",
      "Epoch 78/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0527\n",
      "Epoch 79/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0546\n",
      "Epoch 80/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0540\n",
      "Epoch 81/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0582\n",
      "Epoch 82/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0518\n",
      "Epoch 83/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0537\n",
      "Epoch 84/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 85/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0529\n",
      "Epoch 86/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0515\n",
      "Epoch 87/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0499\n",
      "Epoch 88/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0510\n",
      "Epoch 89/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 90/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0522\n",
      "Epoch 91/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0497\n",
      "Epoch 92/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0491\n",
      "Epoch 93/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0524\n",
      "Epoch 94/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0505\n",
      "Epoch 95/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0504\n",
      "Epoch 96/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0491\n",
      "Epoch 97/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0503\n",
      "Epoch 98/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0503\n",
      "Epoch 99/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0495\n",
      "Epoch 100/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 101/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0473\n",
      "Epoch 102/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0473\n",
      "Epoch 103/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0486\n",
      "Epoch 104/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 105/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0449\n",
      "Epoch 106/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0456\n",
      "Epoch 107/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0479\n",
      "Epoch 108/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 109/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 110/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0477\n",
      "Epoch 111/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0467\n",
      "Epoch 112/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0460\n",
      "Epoch 113/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0452\n",
      "Epoch 114/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0460\n",
      "Epoch 115/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0446\n",
      "Epoch 116/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0467\n",
      "Epoch 117/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0480\n",
      "Epoch 118/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0445\n",
      "Epoch 119/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 120/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 121/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0438\n",
      "Epoch 122/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0452\n",
      "Epoch 123/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0430\n",
      "Epoch 124/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 125/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 126/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 127/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 128/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0430\n",
      "Epoch 129/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 130/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0429\n",
      "Epoch 131/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 132/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0429\n",
      "Epoch 133/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0435\n",
      "Epoch 134/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 135/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0449\n",
      "Epoch 136/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0418\n",
      "Epoch 137/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "Epoch 138/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0446\n",
      "Epoch 139/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0426\n",
      "Epoch 140/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 141/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 142/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 143/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 144/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 145/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 146/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0426\n",
      "Epoch 147/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 148/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 149/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 150/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "result of nn: mape 0.17031512369349014 rmse 62969.8417308269 \n",
      "result of knn: mape 0.15538807076848277 rmse 43259.008731742964 \n",
      "result of adaboost: mape 0.1321956211099811 rmse 31286.410954761817 \n",
      "result of decisiontree: mape 0.15282557924311113 rmse 43259.88726002415 \n",
      "result of randomforest: mape 0.09584193916824381 rmse 27286.541660190578 \n",
      "result of linear: mape 0.12721893939056725 rmse 28207.114743929153 \n",
      "2 fold\n",
      "Epoch 1/150\n",
      "292/292 [==============================] - 1s 1ms/step - loss: 0.6451\n",
      "Epoch 2/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.2196\n",
      "Epoch 3/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1360\n",
      "Epoch 4/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1168\n",
      "Epoch 5/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1104\n",
      "Epoch 6/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1002\n",
      "Epoch 7/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0963\n",
      "Epoch 8/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0916\n",
      "Epoch 9/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 10/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 11/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0873\n",
      "Epoch 12/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0887\n",
      "Epoch 13/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0819\n",
      "Epoch 14/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0867\n",
      "Epoch 15/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0832\n",
      "Epoch 16/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0775\n",
      "Epoch 17/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0816\n",
      "Epoch 18/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0907\n",
      "Epoch 19/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0732\n",
      "Epoch 20/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0736\n",
      "Epoch 21/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0786\n",
      "Epoch 22/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0810\n",
      "Epoch 23/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0710\n",
      "Epoch 24/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0740\n",
      "Epoch 25/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0761\n",
      "Epoch 26/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 27/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0818\n",
      "Epoch 28/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0774\n",
      "Epoch 29/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0751\n",
      "Epoch 30/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0782\n",
      "Epoch 31/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0741\n",
      "Epoch 32/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0725\n",
      "Epoch 33/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0716\n",
      "Epoch 34/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0728\n",
      "Epoch 35/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0675\n",
      "Epoch 36/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0656\n",
      "Epoch 37/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0697\n",
      "Epoch 38/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0677\n",
      "Epoch 39/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0689\n",
      "Epoch 40/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0712\n",
      "Epoch 41/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0700\n",
      "Epoch 42/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0674\n",
      "Epoch 43/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "Epoch 44/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0682\n",
      "Epoch 45/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0670\n",
      "Epoch 46/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0655\n",
      "Epoch 47/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0647\n",
      "Epoch 48/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0624\n",
      "Epoch 49/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0654\n",
      "Epoch 50/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0653\n",
      "Epoch 51/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0642\n",
      "Epoch 52/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0653\n",
      "Epoch 53/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0607\n",
      "Epoch 54/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0655\n",
      "Epoch 55/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0606\n",
      "Epoch 56/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0632\n",
      "Epoch 57/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0664\n",
      "Epoch 58/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0595\n",
      "Epoch 59/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0585\n",
      "Epoch 60/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0589\n",
      "Epoch 61/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 62/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0590\n",
      "Epoch 63/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0621\n",
      "Epoch 64/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0545\n",
      "Epoch 65/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 66/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0583\n",
      "Epoch 67/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0524\n",
      "Epoch 68/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0607\n",
      "Epoch 69/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 70/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0516\n",
      "Epoch 71/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0523\n",
      "Epoch 72/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0525\n",
      "Epoch 73/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0514\n",
      "Epoch 74/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0519\n",
      "Epoch 75/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0524\n",
      "Epoch 76/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0538\n",
      "Epoch 77/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 78/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0527\n",
      "Epoch 79/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0527\n",
      "Epoch 80/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0495\n",
      "Epoch 81/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 82/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0512\n",
      "Epoch 83/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0497\n",
      "Epoch 84/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 85/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0465\n",
      "Epoch 86/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0491\n",
      "Epoch 87/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0487\n",
      "Epoch 88/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0523\n",
      "Epoch 89/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 90/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0482\n",
      "Epoch 91/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 92/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0455\n",
      "Epoch 93/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 94/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0480\n",
      "Epoch 95/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0461\n",
      "Epoch 96/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0487\n",
      "Epoch 97/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0474\n",
      "Epoch 98/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0452\n",
      "Epoch 99/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0465\n",
      "Epoch 100/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0456\n",
      "Epoch 101/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 102/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 103/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0471\n",
      "Epoch 104/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0458\n",
      "Epoch 105/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0462\n",
      "Epoch 106/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 107/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0452\n",
      "Epoch 108/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0435\n",
      "Epoch 109/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0457\n",
      "Epoch 110/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 111/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "Epoch 112/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 113/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0450\n",
      "Epoch 114/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 115/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 116/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0435\n",
      "Epoch 117/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 118/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0435\n",
      "Epoch 119/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 120/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "Epoch 121/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 122/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0434\n",
      "Epoch 123/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 124/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 125/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 126/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 127/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 128/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 129/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 130/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 131/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0434\n",
      "Epoch 132/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 133/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 134/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 135/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 136/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 137/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 138/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 139/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 140/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 141/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 142/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 143/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0422\n",
      "Epoch 144/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "Epoch 145/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 146/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 147/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 148/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 149/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 150/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "result of nn: mape 0.19776970117586357 rmse 48927.58860849709 \n",
      "result of knn: mape 0.17231838024003332 rmse 45811.294991253155 \n",
      "result of adaboost: mape 0.12854424278844875 rmse 37021.04866004405 \n",
      "result of decisiontree: mape 0.1659962687879355 rmse 44926.53989268985 \n",
      "result of randomforest: mape 0.0931391873278092 rmse 32953.46187624086 \n",
      "result of linear: mape 0.14849838926847211 rmse 34489.306845003215 \n",
      "3 fold\n",
      "Epoch 1/150\n",
      "292/292 [==============================] - 1s 1ms/step - loss: 0.6864\n",
      "Epoch 2/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.2438\n",
      "Epoch 3/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1517\n",
      "Epoch 4/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1147\n",
      "Epoch 5/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1061\n",
      "Epoch 6/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1023\n",
      "Epoch 7/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.1047\n",
      "Epoch 8/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0996\n",
      "Epoch 9/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0877\n",
      "Epoch 10/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0900\n",
      "Epoch 11/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0923\n",
      "Epoch 12/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0857\n",
      "Epoch 13/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "Epoch 14/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0853\n",
      "Epoch 15/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0843\n",
      "Epoch 16/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0767\n",
      "Epoch 17/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0835\n",
      "Epoch 18/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0783\n",
      "Epoch 19/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0752\n",
      "Epoch 20/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0799\n",
      "Epoch 21/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0799\n",
      "Epoch 22/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 23/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0799\n",
      "Epoch 24/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0703\n",
      "Epoch 25/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0767\n",
      "Epoch 26/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0690\n",
      "Epoch 27/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0762\n",
      "Epoch 28/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0743\n",
      "Epoch 29/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0722\n",
      "Epoch 30/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0701\n",
      "Epoch 31/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0678\n",
      "Epoch 32/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0731\n",
      "Epoch 33/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0710\n",
      "Epoch 34/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0703\n",
      "Epoch 35/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0690\n",
      "Epoch 36/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0634\n",
      "Epoch 37/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0674\n",
      "Epoch 38/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0703\n",
      "Epoch 39/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0692\n",
      "Epoch 40/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0706\n",
      "Epoch 41/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0668\n",
      "Epoch 42/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0641\n",
      "Epoch 43/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0599\n",
      "Epoch 44/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "Epoch 45/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 46/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 47/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0627\n",
      "Epoch 48/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0594\n",
      "Epoch 49/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0606\n",
      "Epoch 50/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 51/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0625\n",
      "Epoch 52/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0582\n",
      "Epoch 53/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 54/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0601\n",
      "Epoch 55/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 56/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0574\n",
      "Epoch 57/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0541\n",
      "Epoch 58/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0549\n",
      "Epoch 59/150\n",
      "292/292 [==============================] - 0s 1ms/step - loss: 0.0566\n",
      "Epoch 60/150\n",
      " 89/292 [========>.....................] - ETA: 0s - loss: 0.0633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-add8c2354d77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mlinear_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeral_temp_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mmape_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-add8c2354d77>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(numeral_temp_data, i, k_fold)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3417\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m       \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2780\u001b[0;31m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2781\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m       \u001b[0mflat_inputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mflat_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   2821\u001b[0m   \u001b[0;31m# are eventually passed to ConcreteFunction()._call_flat, which requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m   \u001b[0;31m# expanded composites.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m   \u001b[0;31m# Check for NumPy arrays in arguments and convert them to Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0mexpand_composites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_nn(numeral_temp_data,i,k_fold):\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    first_layer_neurons = 512\n",
    "    second_layer_neurons = 512\n",
    "    third_layer_neurons = 512\n",
    "\n",
    "    n_inputs = 38\n",
    "    n_outputs = 1\n",
    "    \n",
    "    \n",
    "    # define encoder\n",
    "\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(first_layer_neurons)(visible)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "    \n",
    "    e = Dense(second_layer_neurons)(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "        \n",
    "    e = Dense(third_layer_neurons)(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "    \n",
    "    \n",
    "    output = Dense(n_outputs, activation='linear')(e)\n",
    "    \n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    loss_function = 'mae'\n",
    "    model.compile(optimizer='adam', loss= loss_function)\n",
    "    callback = callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    validation_set = numeral_temp_data.iloc[i*k_fold:(i+1)*k_fold]\n",
    "    index_list = validation_set.index\n",
    "    training_set = numeral_temp_data.drop(index_list)\n",
    "    \n",
    "    X_train = training_set.iloc[:].drop(columns='SalePrice')\n",
    "    y_train = training_set.iloc[:]['SalePrice']\n",
    "    X_test = validation_set.iloc[:].drop(columns='SalePrice')\n",
    "    y_test = validation_set.iloc[:][\"SalePrice\"]\n",
    "    \n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    data = numeral_temp_data.drop(columns='SalePrice')\n",
    "    target = numeral_temp_data['SalePrice']\n",
    "    \n",
    "    x_scaler.fit(data)\n",
    "    target = target.values.reshape(-1,1)\n",
    "    y_scaler.fit(target)\n",
    "    \n",
    "    X_train = x_scaler.transform(X_train)\n",
    "    y_train = y_scaler.transform(y_train.values.reshape(-1,1))\n",
    "    X_test = x_scaler.transform(X_test)\n",
    "    y_test = y_scaler.transform(y_test.values.reshape(-1,1))\n",
    "    \n",
    "    model.fit(X_train,y_train, epochs=150, batch_size=4)\n",
    "    pred = model.predict(X_test)\n",
    "    pred = y_scaler.inverse_transform(pred)\n",
    "    y_test = y_scaler.inverse_transform(y_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred)**0.5\n",
    "    print(\"result of nn: mape {} rmse {} \".format(mape,rmse))\n",
    "\n",
    "\n",
    "for i in range(fold_num):\n",
    "    print('{} fold'.format(i+1))\n",
    "    temp_data = all_data.iloc[:]\n",
    "    numeral_temp_data = temp_data._get_numeric_data()\n",
    "    numeral_temp_data = numeral_temp_data.drop(columns='Id')\n",
    "    numeral_temp_data['Street'] = temp_data['Street']\n",
    "    numeral_temp_data['CentralAir'] = temp_data['CentralAir']\n",
    "    \n",
    "    numeral_temp_data.Street[numeral_temp_data.Street == 'Pave'] = 1\n",
    "    numeral_temp_data.Street[numeral_temp_data.Street == 'Grvl'] = 0\n",
    "    \n",
    "    numeral_temp_data.CentralAir[numeral_temp_data.CentralAir == 'Y'] = 1\n",
    "    numeral_temp_data.CentralAir[numeral_temp_data.CentralAir == 'N'] = 0\n",
    "    numeral_temp_data = numeral_temp_data.fillna(0)\n",
    "    \n",
    "    validation_set = numeral_temp_data.iloc[i*k_fold:(i+1)*k_fold]\n",
    "    index_list = validation_set.index\n",
    "    training_set = numeral_temp_data.drop(index_list)\n",
    "    \n",
    "    X_train = training_set.iloc[:].drop(columns='SalePrice')\n",
    "    y_train = training_set.iloc[:]['SalePrice']\n",
    "    X_test = validation_set.iloc[:].drop(columns='SalePrice')\n",
    "    y_test = validation_set.iloc[:][\"SalePrice\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    knn = KNeighborsRegressor()\n",
    "    adaboost = AdaBoostRegressor()\n",
    "    decisiontree = DecisionTreeRegressor()\n",
    "    randomforest = RandomForestRegressor()\n",
    "    linear = LinearRegression()\n",
    "    \n",
    "    knn.fit(X_train,y_train)\n",
    "    adaboost.fit(X_train,y_train)\n",
    "    decisiontree.fit(X_train,y_train)\n",
    "    randomforest.fit(X_train,y_train)\n",
    "    linear.fit(X_train,y_train)\n",
    "    \n",
    "    knn_pred = knn.predict(X_test)\n",
    "    adaboost_pred = adaboost.predict(X_test)\n",
    "    decisiontree_pred = decisiontree.predict(X_test)\n",
    "    randomforest_pred = randomforest.predict(X_test)\n",
    "    linear_pred = linear.predict(X_test)\n",
    "    \n",
    "    train_nn(numeral_temp_data,i,k_fold)\n",
    "    \n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    name_list = ['Knn','Adaboost','Decisiontree','Randomforest','Linear']\n",
    "    \n",
    "    knn_mape = mean_absolute_percentage_error(knn_pred, y_test)\n",
    "    adaboost_mape = mean_absolute_percentage_error(adaboost_pred, y_test)\n",
    "    decisiontree_mape = mean_absolute_percentage_error(decisiontree_pred, y_test)\n",
    "    randomforest_mape = mean_absolute_percentage_error(randomforest_pred, y_test)\n",
    "    linear_mape = mean_absolute_percentage_error(linear_pred, y_test)\n",
    "    \n",
    "    mape_list = [knn_mape, adaboost_mape, decisiontree_mape, randomforest_mape, linear_mape]\n",
    "    \n",
    "    knn_rmse = mean_squared_error(knn_pred, y_test) ** 0.5\n",
    "    adaboost_rmse = mean_squared_error(adaboost_pred, y_test) ** 0.5\n",
    "    decisiontree_rmse = mean_squared_error(decisiontree_pred, y_test) ** 0.5\n",
    "    randomforest_rmse = mean_squared_error(randomforest_pred, y_test) ** 0.5\n",
    "    linear_rmse = mean_squared_error(linear_pred, y_test) ** 0.5\n",
    "    \n",
    "    rmse_list = [knn_rmse, adaboost_rmse, decisiontree_rmse, randomforest_rmse, linear_rmse]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plt.subplot(2,1,1)\n",
    "    #plt.bar(name_list,mape_list,color='green')\n",
    "    #plt.xlabel(\"MAPE\")\n",
    "    \n",
    "    #plt.subplot(2,1,2)\n",
    "    #plt.bar(name_list,rmse_list,color='blue')\n",
    "    #plt.xlabel(\"RMSE\")\n",
    "    #plt.show()\n",
    "    \n",
    "    print(\"result of knn: mape {} rmse {} \".format(knn_mape,knn_rmse))\n",
    "    print(\"result of adaboost: mape {} rmse {} \".format(adaboost_mape,adaboost_rmse))\n",
    "    print(\"result of decisiontree: mape {} rmse {} \".format(decisiontree_mape,decisiontree_rmse))\n",
    "    print(\"result of randomforest: mape {} rmse {} \".format(randomforest_mape,randomforest_rmse))\n",
    "    print(\"result of linear: mape {} rmse {} \".format(linear_mape,linear_rmse))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5339883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape(before drop outliers): 0.12721893939058315\n",
      "rmse(before drop outliers): 28207.114743929003\n",
      "mape(after drop outliers): 0.11733509522454112\n",
      "rmse(after drop outliers): 31826.709442712443\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from scipy import stats\n",
    "\n",
    "#show feature boxplot\n",
    "features = numeral_temp_data.columns\n",
    "features = features.tolist()\n",
    "features.remove('Street')\n",
    "features.remove('CentralAir')\n",
    "#for i in features:\n",
    "#    print(i)\n",
    "#    z = np.abs(stats.zscore(temp_data[i]))\n",
    "#    plt.boxplot(z)\n",
    "#    plt.show()\n",
    "    \n",
    "#before remove outlier\n",
    "validation_set = numeral_temp_data.iloc[0*k_fold:(1)*k_fold]\n",
    "index_list = validation_set.index\n",
    "training_set = numeral_temp_data.drop(index_list)\n",
    "\n",
    "X_train = training_set.iloc[:].drop(columns='SalePrice')\n",
    "y_train = training_set.iloc[:]['SalePrice']\n",
    "X_test = validation_set.iloc[:].drop(columns='SalePrice')\n",
    "y_test = validation_set.iloc[:][\"SalePrice\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(pred, y_test)\n",
    "rmse = mean_squared_error(pred, y_test) ** 0.5\n",
    "print(\"mape(before drop outliers):\",mape)\n",
    "print(\"rmse(before drop outliers):\",rmse)\n",
    "\n",
    "#after remove outlier(saleprice)\n",
    "percentile25 = y_train.quantile(0.25)\n",
    "percentile75 = y_train.quantile(0.75)\n",
    "\n",
    "iqr = percentile75 - percentile25\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "\n",
    "upper_outlier_index = y_train[y_train > upper_limit].index\n",
    "lower_outlier_index = y_train[y_train < lower_limit].index\n",
    "\n",
    "X_train = X_train.drop(index=upper_outlier_index)\n",
    "X_train = X_train.drop(index=lower_outlier_index)\n",
    "y_train = y_train.drop(index=upper_outlier_index)\n",
    "y_train = y_train.drop(index=lower_outlier_index)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(pred, y_test)\n",
    "rmse = mean_squared_error(pred, y_test) ** 0.5\n",
    "print(\"mape(after drop outliers):\",mape)\n",
    "print(\"rmse(after drop outliers):\",rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "038605e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fb6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
